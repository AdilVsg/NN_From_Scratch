# MNIST-FROM-SCRATCH

![Author](https://img.shields.io/badge/Author-Adil%20CHOUKAIRE-blue)
![Project](https://img.shields.io/badge/Project-Computer%20Vision-blueviolet)
![Project](https://img.shields.io/badge/Project-Deep%20Learning-blueviolet)
![Status](https://img.shields.io/badge/Status-Completed-success)

![Python](https://img.shields.io/badge/Python-3.13%2B-blue)
![NumPy](https://img.shields.io/badge/NumPy-Core%20Maths-013243)
![Pandas](https://img.shields.io/badge/Pandas-Data-150458)

Deep Learning solution designed to understand the fundamental mathematics of Artificial Intelligence by building a Neural Network entirely from scratch.

This project implements a Multi-Layer Perceptron (MLP) using raw NumPy matrix operations, bypassing high-level frameworks like TensorFlow or PyTorch to perform handwritten digit classification.

---

## ğŸ“ Project Structure

```bash
NN_FROM_SCRATCH/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ train.csv   # MNIST Training Data (Labels + Pixels)
â”‚   â””â”€â”€ test.csv    # MNIST Test Data (Pixels only)
â”œâ”€â”€ ğŸ“ venv/        # Virtual Environment
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ submission.csv
â””â”€â”€ README.md
```

âš ï¸ **Dataset Source:**  
The `train.csv` and `test.csv` files are derived from the famous MNIST dataset (via the Kaggle Digit Recognizer competition). Ensure these files are located in the `data/` directory before running the script.

---

## ğŸ¯ Objective

The goal is to correctly classify grayscale images of handwritten digits (0â€“9) based on pixel intensity.

- **Input:** 784 pixels (28x28 flattened image)  
- **Output:** A probability distribution across 10 classes (0â€“9)  
- **Metric:** Categorical Accuracy  
- **Challenge:** The project prohibits the use of automatic differentiation engines. All gradients, derivatives (ReLU, Softmax), and backpropagation algorithms are computed and implemented manually.

---

## ğŸš€ Installation & Usage

### Clone the repository

```bash
git clone https://github.com/your-username/nn-from-scratch.git
cd nn-from-scratch
```

### Install dependencies

```bash
pip install -r requirements.txt
```

### Run the pipeline

```bash
python main.py
```

---

## ğŸ› ï¸ Script Description

### `main.py`

This is the core script containing the entire pipeline. It implements a Vanilla Neural Network architecture designed for mathematical transparency.

---

### 1ï¸âƒ£ Data Preparation & Engineering

- **Normalization:** Pixel values are scaled from `[0, 255]` to `[0, 1]` to prevent gradient explosion.  
- **Transposition:** Data is reshaped to `(784, m)` to facilitate vectorized matrix operations.  
- **One-Hot Encoding:** Converts categorical labels (`Y`) into binary vectors for the Loss calculation.

---

### 2ï¸âƒ£ Model Architecture (The MLP)

- **Input Layer:** 784 Neurons  
- **Hidden Layer:** 128 Neurons using ReLU activation (rectifies non-linearity)  
- **Output Layer:** 10 Neurons using Softmax activation (converts logits to probabilities)  
- **Initialization:** Weights are initialized using a scaled Random Normal distribution (`randn * 0.01`) to break symmetry  

---

### 3ï¸âƒ£ The Mathematical Engine

#### Forward Propagation

- Computes linear combinations:  
  ```
  Z = W Â· X + b
  ```
- Applies activation functions  
- Includes a **Numerically Stable Softmax** to handle large exponentials  

#### Backward Propagation

Manually calculates gradients using the Chain Rule:

- Computes error at the output (`âˆ‚Zâ‚‚`)  
- Backpropagates error to the hidden layer (`âˆ‚Zâ‚`) using the derivative of ReLU  

#### Optimization

- Updates parameters (`W`, `b`) using standard **Gradient Descent**  
- Fixed learning rate  

---

### 4ï¸âƒ£ Output & Submission

The script generates a final output file named:

```
submission.csv
```

This CSV contains:

- `ImageId`
- Predicted `Label` for the unseen test set  

The file follows the submission format required by the Kaggle platform.

---

## âš ï¸ Notes

### ğŸ“Š Performance

- Training accuracy: **~95.8%**
- Achieved after **3000 iterations**

### âš™ï¸ Resource Usage

- Optimized for **CPU execution**
- Uses vectorized NumPy operations (SIMD)
- Completes training in a few minutes

### ğŸ”§ Hyperparameters

- Learning rate: **0.1**
- Hidden units: **128**
- Configured for optimal convergence on this specific dataset